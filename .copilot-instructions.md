# Instructions for AI Assistants

## Version Management - CRITICAL

**DO NOT manually bump versions in these files:**
- `porpoise-ui/package.json`
- `Porpoise.Api/Porpoise.Api.csproj`
- `VERSION.md`

**Why:** CI/CD automatically bumps versions when merging to main. Manual version changes create merge conflicts.

## Git Workflow - SIMPLIFIED

**User's Daily Workflow (AI handles all commits to develop):**

When user says "commit and push to develop":
```bash
git add -A
git commit -m "descriptive message"
git push origin develop
```

- ‚úÖ No terminal prompts required
- ‚úÖ No user input needed
- ‚úÖ No version file editing
- ‚úÖ Just commit and push

**Production Releases (User uses GitHub UI):**

When ready to deploy to production:
1. User goes to GitHub.com ‚Üí Pull Requests
2. Creates PR: `base: main` ‚Üê `compare: develop`
3. Merges through GitHub UI
4. CI/CD automatically:
   - Bumps version numbers in package.json and .csproj
   - Creates version tag
   - Builds and deploys

**Never:**
- ‚ùå Don't merge develop ‚Üí main via command line
- ‚ùå Don't manually edit version numbers
- ‚ùå Don't commit directly to main

**If VS Code shows tag conflicts:**
- User should click "Replace Local Tag(s)" to sync with remote
- This is normal - CI/CD creates tags automatically
git checkout --theirs VERSION.md porpoise-ui/package.json Porpoise.Api/Porpoise.Api.csproj
git add VERSION.md porpoise-ui/package.json Porpoise.Api/Porpoise.Api.csproj
git commit --no-edit  # Hook skips version check for merge commits

## Common Pitfalls

- ‚ùå Don't manually change version numbers
- ‚ùå Don't merge develop ‚Üí main via command line (use GitHub UI instead)
- ‚ùå Don't commit directly to main branch
- ‚úÖ Always work on develop branch
- ‚úÖ Let CI/CD handle all versioning
- ‚úÖ Use GitHub UI for production releases (develop ‚Üí main)
- ‚úÖ If VS Code prompts about tag conflicts, user clicks "Replace Local Tag(s)"

## Development Server Management

- **DO NOT restart dev servers** - The user already has servers running (npm run dev, dotnet watch)
- Vite hot-reloads automatically for file changes
- Only suggest restarting if there's a genuine build error or dependency change

## User Preferences

- **Git expertise:** User prefers not to learn complex git commands - keep it simple
- **Bedtime:** 11:00 PM - Remind user to wrap up work around this time
- **Work style:** Prefers iterative development with testing before commits

## Product Design Philosophy - CRITICAL FOR SUCCESS ‚≠ê

**Context:** 
- Original Porpoise PC (2013) and Orca (2014) were desktop apps that didn't achieve market adoption
- User (Dan) was heavily influenced by Val Smith's design preferences during original build
- **Val Smith background:**
  - PhD in Statistics, taught at California State University
  - Academic perspective (rigor/methodology focus) vs commercial perspective (ease of use/adoption)
  - May have invented/adopted the "index" concept used in Porpoise (ego investment in his innovations)
  - Has strong opinions about "proper" survey analysis methodology
- **Dan's position:**
  - Software developer, NOT a survey analysis professional
  - Has relied on Val for domain expertise and market perspective
  - Now questioning whether Val's academic lens hurt commercial viability
  - Concerned he unknowingly propagated design decisions that limit market appeal
- The apps didn't sell and were not widely adopted
- Val believes it was because they weren't web-based; Dan is not convinced that's the only reason
- Currently rebuilding as web app with goal of actual market success
- **Target market:** Consultants and companies doing survey analysis (not academics)
- **Dan's request:** Help distinguish between "academically rigorous" and "commercially viable"

**CRITICAL DIRECTIVE: Question Everything**

When designing features for the new web app:

1. **Challenge the Original Design**
   - ‚ùå Don't blindly recreate legacy features just because they existed
   - ‚ùå Don't assume Val's original design choices were optimal
   - ‚ùå Don't assume "desktop vs web" was the only barrier to adoption
   - ‚úÖ Ask: "Would the AVERAGE user understand this?"
   - ‚úÖ Ask: "Does this clearly save time/effort compared to alternatives?"
   - ‚úÖ Ask: "Is there a simpler/better way to achieve the same goal?"

2. **Design for Market Success**
   - Target: Average prospective client (not power users or experts)
   - Goal: User immediately sees how product saves time and effort
   - Prioritize: Intuitive, obvious, simple over powerful, flexible, complex
   - Validate: Would a new user "get it" in 5 minutes?

3. **When Reviewing Legacy Features**
   - Question the necessity of each feature
   - Propose modern alternatives or simplifications
   - Suggest phased approaches (MVP ‚Üí enhancements)
   - Flag complexity that might confuse users
   - Recommend user testing before building

4. **Red Flags to Watch For**
   - Features that require extensive training/documentation
   - UI patterns that aren't common in modern web apps
   - Workflows that have many steps or require jumping between screens
   - Terminology/jargon that average users wouldn't know
   - Features that only 10% of users would use 90% of the time

5. **When in Doubt**
   - Propose the simpler version first
   - Suggest A/B testing or user validation
   - Recommend building MVP and iterating based on feedback
   - Ask: "Could we achieve 80% of the value with 20% of the complexity?"

6. **Academic vs Commercial Design Decisions**
   - **Academic lens (Val's perspective):**
     - Methodological rigor and statistical correctness
     - Complete feature sets for edge cases
     - Terminology precise to the field
     - Assumes users have statistics background
   - **Commercial lens (what we need):**
     - User can accomplish task without training
     - Handles 90% use cases simply, 10% can be advanced
     - Terminology that average business user understands
     - Assumes users want results, not to learn statistics
   
   **When evaluating Val's input:**
   - Ask: "Is this necessary for statistical validity, or just 'proper methodology'?"
   - Ask: "Would an MBA doing market research understand this, or only a PhD?"
   - Ask: "What do competitor tools (Qualtrics, SurveyMonkey, Tableau) do?"
   - Ask: "Is this solving a real pain point, or teaching 'the right way' to do it?"
   
   **Red flags for academic over-design:**
   - "This is how survey researchers should do it"
   - "Other tools oversimplify this"
   - "Users need to understand the statistical foundation"
   - Features named after statistical concepts (Chi-square, Cram√©r's V, etc.)
   - Multiple options to accommodate different methodological schools of thought

**Remember:** The goal is not to recreate the original apps, but to build something that will actually be adopted and succeed in the market. Every design decision should be justified by user value, not legacy precedent or academic rigor.

**Dan's Specific Need:** Help distinguish between Val's expert domain knowledge (valuable) and his academic design preferences (potentially limiting). Question assumptions, propose commercial alternatives, focus on pain-point-solving over methodology-teaching.

## Intended Workflow & Key Assumptions - CRITICAL ‚ö†Ô∏è

**Dan's Vision for Data Import:**
1. **User starts with TWO files:**
   - Data file (CSV or Excel) - actual survey responses
   - Survey definition file - questions, response labels, possibly blocks
   - First row of data file contains question numbers
   
2. **Auto-matching process:**
   - Match question numbers from data file to survey definition file
   - Auto-populate question text from survey file
   - Auto-populate response labels from survey file
   - Auto-detect/apply block groupings if present
   - Goal: **Minimize manual typing** - user just validates/confirms

3. **Value proposition:**
   - Skip the tedious manual data entry from Orca workflow
   - Import existing survey definitions (users often already have these)
   - Reduce errors from manual transcription
   
**WAITING ON:** Sample files from Bereket (survey + data file examples)

**Open Questions to Validate:**

**About Blocks:**
- ‚ùì Do questions in a block HAVE to have the same number of responses? (Need verification)
- ‚ùì Do users actually understand what blocks are for?
- ‚ùì Do users see value in the block concept?
- ‚ùì Is this a "nice to have" or essential for analysis?
- ‚ùì Could we achieve same goal with simpler "question grouping"?
- üö© **RED FLAG:** If users don't understand blocks, they won't use them correctly
- üí° **ALTERNATIVE:** Could we auto-detect blocks from survey structure instead of requiring manual setup?

**About Survey File Import:**
- ‚ùì What format are survey definition files typically in?
- ‚ùì Do all clients have survey definition files, or is this only for certain tools?
- ‚ùì What if data file has question numbers but no separate survey file?
- ‚ùì Should we support manual entry as fallback?

**Design Implication:**
- This workflow is FUNDAMENTALLY DIFFERENT from Orca's approach
- Orca: Import messy data ‚Üí manually define everything ‚Üí export
- New vision: Import pre-defined survey + data ‚Üí auto-match ‚Üí validate
- **Focus should be on SMART IMPORT, not manual definition UI**
- Manual definition becomes "fallback" or "fix mismatches" not primary workflow

## Strategic Design Approach: Progressive Disclosure ("Dumb It Down" with Depth)

**Core Principle:** Simple surface, sophisticated depth

**Layer 1 - Casual User Interface (Default View):**
- Plain language, no jargon
- Obvious next steps
- Hide statistical complexity
- Show results in business terms ("this difference is meaningful" not "p < 0.05")
- Auto-detect and auto-configure where possible
- Assume user wants answers, not to learn statistics

**Layer 2 - Advanced Options (Progressive Disclosure):**
- "Advanced" or "Show Details" toggles
- Statistical measures in collapsible sections
- Configuration options for power users
- Methodology explanations available but not front-and-center

**Layer 3 - Statistical Integrity (Transparent but Hidden):**
- Full statistical reports downloadable
- Chi-square, Cram√©r's V, p-values in "Technical Details" panel
- Methodology documentation linked from help
- Satisfy the statistician without intimidating the marketer

**Examples:**
- ‚ùå Don't: "Set data type: Nominal/Ordinal/Scale" (front and center)
- ‚úÖ Do: Auto-detect, show "Advanced settings" link if user needs to override
- ‚ùå Don't: Show Chi-square value prominently
- ‚úÖ Do: Show "‚≠ê‚≠ê‚≠ê Highly significant" with "See statistical details" link
- ‚ùå Don't: Require user to configure blocks manually
- ‚úÖ Do: "We found 5 related questions. Group them together?" with auto-detect

**Goal:** 
- Casual user accomplishes task without knowing statistics exists
- Expert user can drill down to verify statistical rigor
- Product is both accessible AND credible
- Marketing appeal (easy) + academic credibility (rigorous)



